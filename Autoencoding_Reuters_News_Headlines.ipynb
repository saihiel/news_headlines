{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Autoencoding_Reuters_News_Headlines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZwdJ0VphZ53",
        "colab_type": "text"
      },
      "source": [
        "# Reuters News Headlines\n",
        "This project focuses on the application of deep learning to natural language processing.\n",
        "I will be working with a subset of Reuters news headlines that are collected over 15 months,\n",
        "covering all of 2019, plus a few months in 2018 and in a few months of this year.\n",
        "\n",
        "In particular, I will be building an **autoencoder** of news headlines.\n",
        "I will have an **encoder** that maps a news headline to a vector embedding, and then a **decoder** that reconstructs\n",
        "the news headline. Both our encoder and decoder networks will be Recurrent Neural Networks,\n",
        "I will build: \n",
        "\n",
        "- a neural network that takes a sequence as an input\n",
        "- a neural network that generates a sequence as an output\n",
        "\n",
        "This project is organized as follows:\n",
        "\n",
        "- Exploring the data\n",
        "- Building the autoencoder\n",
        "- Training the autoencoder using *data augmentation*\n",
        "- Analyzing the embeddings (interpolating between headlines)\n",
        "\n",
        "Furthermore, I use **data augmentation** for improving of\n",
        "the robustness of the autoencoder, as proposed by Shen et al [1].\n",
        "\n",
        "[1] Shen et al (2019) \"Educating Text Autoencoders: Latent Representation Guidance via Denoising\" https://arxiv.org/pdf/1905.12777.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O36sfBclhZ54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuqjU_EvhZ59",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "First I upload the files `reuters_train.txt` and `reuters_valid.txt` to Google Drive so I can access them through Google Colab easily.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArIvACvehZ6A",
        "colab_type": "code",
        "outputId": "4c952ae2-1797-4276-c14f-b4372b5c2adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUfH-tsFh1jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = '/content/drive/My Drive/reuters_train.txt'\n",
        "valid_path = '/content/drive/My Drive/reuters_valid.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nFbPh_RhZ6D",
        "colab_type": "text"
      },
      "source": [
        "I'll be using a `TabularDataset` to load my data, which works well on structured\n",
        "CSV data with fixed columns (e.g. a column for the sequence, a column for the label). My tabular dataset\n",
        "is even simpler: I have no labels, just some text. So, I am treating my data as a table with one field\n",
        "representing the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoOCBk6ohZ6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "\n",
        "# Tokenization function to separate a headline into words\n",
        "def tokenize_headline(headline):\n",
        "    \"\"\"Returns the sequence of words in the string headline. I also\n",
        "    prepend the \"<bos>\" or beginning-of-string token, and append the\n",
        "    \"<eos>\" or end-of-string token to the headline.\n",
        "    \"\"\"\n",
        "    return (\"<bos> \" + headline + \" <eos>\").split()\n",
        "\n",
        "# Data field (column) representing our *text*.\n",
        "text_field = torchtext.data.Field(\n",
        "    sequential=True,            # this field consists of a sequence\n",
        "    tokenize=tokenize_headline, # how to split sequences into words\n",
        "    include_lengths=True,       # to track the length of sequences, for batching\n",
        "    batch_first=True,           # similar to batch_first=True in nn.RNN demonstrated in lecture\n",
        "    use_vocab=True)             # to turn each character into an integer index\n",
        "train_data = torchtext.data.TabularDataset(\n",
        "    path=train_path,                # data file path\n",
        "    format=\"tsv\",                   # fields are separated by a tab\n",
        "    fields=[('title', text_field)]) # list of fields (I have only one)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njwPSEF1hZ6G",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Histograms of the number of words per headline in our training set.\n",
        "Excluding the `<bos>` and `<eos>` tags in my computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6abSBDNohZ6H",
        "colab_type": "code",
        "outputId": "bb89100d-78e0-43b2-909e-a033b088cbc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "print(len(train_data))\n",
        "\n",
        "print(train_data[5].title)\n",
        "lens = [len(example.title) - 2 for example in train_data]\n",
        "plt.hist(lens, bins = [i for i in range(max(lens))], color=\"orange\")\n",
        "plt.title('Number of words per headline in training set')\n",
        "plt.xlabel('Number of words per headline')\n",
        "plt.ylabel('Count')\n",
        "plt.show\n",
        "\n",
        "# I found this histogram useful to see the distribution of\n",
        "# headline lengths. This allowed me to make decisions about the kind of model \n",
        "# I should use. For instance, the size of the hidden layer/the embedding size\n",
        "# depends on the length of the sequences. Since, if a single headline can be 200 words \n",
        "# long, then the size of the embedding needs to be large enough to \n",
        "# to represent this headline."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "171443\n",
            "['<bos>', 'u.s.', 'navy', 'pursuing', 'block', 'buy', 'of', 'two', 'aircraft', 'carriers', '-', 'senator', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5gcVZ3/8feHQMJNSJCYDSQShKgbvCAMN8E1gkBA3aAP8ANUggJBBYVdcQH1EUTYxVWUZX+AixIICoSsgISLhohAQH8hGSACISIDJCYhkEgIFxEw8P39cc4kZdM901OZnk5PPq/n6WeqT1WdOqeqpr9d51SfUkRgZmZWxgbNLoCZmbUuBxEzMyvNQcTMzEpzEDEzs9IcRMzMrDQHETMzK81BZD0g6QpJ5zRp25J0uaTnJM1uRhlyOcZKWtys7dci6U5Jx/XRtkLSjnl69Tkh6UOSHm3QNl+S9I5G5F3n9udJGtvby9oaGza7AOsjSQuATYHtI+IvOe044DMRMbaJRWuEfYD9gRGddbV1S0TcDbyrQXlvXmY9SaOAJ4GNImLVWmx/p0Ys2xckXQEsjohvNrssXfGVSPMMAE5udiF6StKAHq6yHbCgLwOIpHXyy1GJfWddWFeP8/rGQaR5vgecKmlw5QxJo3LTw4aFtNXNHpKOkfRbST+UtFLSE5I+mNMXSVomaUJFtltLmiHpRUl3SdqukPe787wVkh6VdHhh3hWSLpF0q6S/AB+pUt5tJE3L63dIOj6nHwv8BNgrN2t8u8q6CyXtmqc/neu9U+f6kn6RpwdJukDSU/l1gaRBed5YSYslnSbpaeBySZvksj8n6RFgt4rtniZpSd4fj0rar9pBynn8qFH7LtsuH88XJd0maetCHntK+l0+zr8vNrdI+pyk+Xm9JySdUFH2r0lamvfX52ts+01NfZIWSDpV0oOSnpd0raSNC/M/LmluLtPvJL2vi7wrm9AuknRLLvO9knaoserM/HdlPnf2qjjvnwXOkrSDpN9IelbSnyVdpcL/VK7LR/P0WZKmSroyb3+epLaSy+4i6YE873/zPqraZCxpx3zePJ/LeG1hXtXzR9JE4NPAv+X631RrHzddRPjVxy9gAfBR4HrgnJx2HHBnnh4FBLBhYZ07gePy9DHAKuBzpCuac4A/ARcBg4ADgBeBzfPyV+T3/5Tn/xdwT563GbAo57Uh8AHgz8CYwrrPA3uTvnRsXKU+M4GLgY2BnYHlwL6Fst7Txb64Evhqnr4UeBz4YmHev+Tps4FZwNuAocDvgO/keWPz/vhurt8mwHnA3cBWwEjgYVLTAKSmm0XANoX9vUON8jV6392Z6/zOXO47gfPyvG2BZ4GD8/r75/dD8/yPATsAAj4MvAzskueNA54B3pPLeTXpnNqxULZzCvtvccX5ORvYJu+/+cAX8rwPAMuAPUjn3oS8/KAa+69ym88Cu+f9dRUwpcZ6o3jz/8Ax+Th/Oa+/CbBj3i+DSOfFTOCCyv+1PH0W8ErenwOA/wBm9XRZYCCwkNSSsBHwKeC1zv1ZpS7XAN/oPAeAfXpw/lTNc116+Uqkub4FfFnS0BLrPhkRl0fE68C1pA/KsyPi1Yi4jXRS71hY/paImBkRr5JO6L0kjQQ+TmpuujwiVkXEA8B1wGGFdW+MiN9GxBsR8UqxEDmPvYHTIuKViJhLuvo4us563EX6AAT4EOmftfP9h/N8SN/Kzo6IZRGxHPg28NlCPm8AZ+b6/xU4HDg3IlZExCLgwsKyr5M+dMZI2igiFkTE412UsSH7ruDyiPhjLvdUUiAG+Axwa0TcmtefAbSTPtiIiFsi4vFI7gJuy/uQXP/LI+LhSE2JZ3VRv2oujIinImIFcFOhTBOB/4mIeyPi9YiYDLwK7FlnvjdExOxI/RxXFfKt11MR8d95f/81IjoiYkY+7suBH7Dm/Knmnrw/Xwd+Cry/xLJ7kj70L4yIv0XE9aSgW8vfSM262+T/kXtyej3nzzrPQaSJIuJh4Gbg9BKrP1OY/mvOrzKt2Km5qLDdl4AVpG+a2wF75KaJlZJWkj6w/6HaulVsA6yIiBcLaQtJ36LrcRfwIUnDSd/4pgJ7K3WsbgnMLWxnYcU2tim8X17xIb1NRblXrxsRHcAppA/WZZKmSCrmValR+67T04Xpl1lz3LYDDqvIfx9gOICkgyTNyk0hK0nBpbMprGb969RVmb5aUaaR/P2xKJNvvf5uf0oalo/fEkkvAD9jzT6oZ/sbq3bfSq1ltwGWRL5cqFauCv9GulqcnZvFOpsW6zl/1nkOIs13JnA8f/+h29kJvWkhbW1PrJGdE5I2JzVTPEU6+e+KiMGF1+YR8cXCul0N9fwUsJWktxTS3g4sqadQ+QP9ZVITxcyIeIH0zzuR9E3wjcJ2tius+vacVquMSynUOS9f3O7VEbFPzjNITWG1NGrfdWcR8NOK/DeLiPOU+oOuA74PDIuIwcCtpA8r6Kb+a1mmcyvKtGlEXNNL+Xeqtd8q0/89p703IrYgXb3pTWv1rqXAtpKK2xlZa+GIeDoijo+IbYATgItzP1F3509LDLHuINJk+UP0WuArhbTlpA/hz0gakL+51OqArNfBkvaRNBD4Dql9dxHpSuidkj4raaP82k3SP9ZZ/kWk/on/kLRx7mQ9lvSNsF53ASexpunqzor3kNqVvylpqFLH87e62cZU4AxJQySNIAUpACS9S9K++YP4FdJV2xs18oEG7bs6/Az4hKQD83mwsVIn+AhSu/wgUv/TKkkHkfrCivU/RtIYSZuSvqz0hh8DX5C0h5LNJH2s4ktEb1hOOibd/cbkLcBLwPOStgW+1svlqOb/kZpET5K0oaTxpH6eqiQdlo8ZwHOk4PAG3Z8/z9B9/ZvOQWTdcDapk63oeNI/xLPATqQP6rVxNemDZAWwK+kbG7kZ6gDgCNK366dZ00FdryNJHaFPATeQ+iZ+3YP17yJ9GMys8R7SzQPtwIPAQ8D9Oa2Wb5OacJ4k9RX8tDBvEKnj/c+k+r4NOKOLvBq572rKgWo88HXSh+oi0jmxQd72V0jB4jngKGBaYd1fAhcAvwE68t/eKFM76dz8v3m7HaQO714VES8D5wK/zU09tfpcvg3sQrqB4RbSzSoNFRGvkTrTjwVWks6Hm0l9Q9XsBtwr6SXSMTo5Ip6o4/y5jNRvt1L5LsV1kf6+Wc/MitQiP/iy5pJ0L/CjiLi82WXpa74SMTPrIUkflvQPuTlrAvA+4FfNLlcz+BefZmY99y5SU+JmwBPAoRGxtLlFag43Z5mZWWluzjIzs9LWu+asrbfeOkaNGtXsYpiZtZT77rvvzxHxptE11rsgMmrUKNrb25tdDDOzliKp6qgHbs4yM7PSHETMzKw0BxEzMyvNQcTMzEpzEDEzs9IaFkTyiKOzlR7pOU/50aiStld6LGaH0iMlB+b0Qfl9R54/qpDXGTn9UUkHFtLH5bQOSWWeyWFmZmuhkVcir5Iekfp+0tPLxuWROL8L/DAidiSNAnpsXv5Y4Lmc/sO8HJLGkEa53In0yM+L87DYA0iPgz0IGAMcmZc1M7M+0rAgkh/Z+VJ+u1F+BbAv8POcPhk4JE+Pz+/J8/fLD30ZT3oO86sR8SRp6Ond86sjD6n8GjAlL2tmZn2koX0i+YphLrAMmAE8DqzMz1cGWMyaJ/ptS37EZJ7/PPDWYnrFOrXSzcysjzT0F+v5Afc7SxpMeljRuxu5vVokTSQ9bpW3v723nhJq/crVPXii6lEetNSsU5/cnRURK4E7gL2Awflh9wAjWPMs7iXk5xTn+VuSnuq3Or1inVrp1bZ/aUS0RUTb0KFvGvrFzMxKauTdWUPzFQiSNgH2B+aTgsmhebEJwI15elp+T57/m0jj1E8Djsh3b20PjAZmA3OA0flur4GkzvfVjwc1M7PGa2Rz1nBgcr6LagNgakTcLOkRYIqkc4AHSM8RJv/9qaQO0rOsjwCIiHmSpgKPAKuAE3MzGZJOAqYDA4BJETGvgfUxM7MK691Dqdra2sKj+NqbuE/ErEuS7ouItsp0/2LdzMxKcxAxM7PSHETMzKw0BxEzMyvNQcTMzEpzEDEzs9IcRMzMrDQHETMzK81BxMzMSmvoKL5mTdeTX6KbWY/5SsTMzEpzEDEzs9IcRMzMrDQHETMzK81BxMzMSnMQMTOz0hxEzMysNAcRMzMrzUHEzMxKcxAxM7PSPOyJWU/1ZCiVo6Jx5TBbB/hKxMzMSnMQMTOz0hxEzMysNAcRMzMrzUHEzMxKa1gQkTRS0h2SHpE0T9LJOf0sSUskzc2vgwvrnCGpQ9Kjkg4spI/LaR2STi+kby/p3px+raSBjaqPmZm9WSOvRFYBX42IMcCewImSxuR5P4yInfPrVoA87whgJ2AccLGkAZIGABcBBwFjgCML+Xw357Uj8BxwbAPrY2ZmFRoWRCJiaUTcn6dfBOYD23axynhgSkS8GhFPAh3A7vnVERFPRMRrwBRgvCQB+wI/z+tPBg5pTG3MzKyaPukTkTQK+ABwb046SdKDkiZJGpLTtgUWFVZbnNNqpb8VWBkRqyrSq21/oqR2Se3Lly/vhRqZmRn0QRCRtDlwHXBKRLwAXALsAOwMLAXOb3QZIuLSiGiLiLahQ4c2enNmZuuNhg57ImkjUgC5KiKuB4iIZwrzfwzcnN8uAUYWVh+R06iR/iwwWNKG+WqkuLyZmfWBRt6dJeAyYH5E/KCQPryw2CeBh/P0NOAISYMkbQ+MBmYDc4DR+U6sgaTO92kREcAdwKF5/QnAjY2qj5mZvVkjr0T2Bj4LPCRpbk77Ounuqp2BABYAJwBExDxJU4FHSHd2nRgRrwNIOgmYDgwAJkXEvJzfacAUSecAD5CClpmZ9RGlL/Trj7a2tmhvb292Mayv9GTE3UbwKL7WT0i6LyLaKtP9i3UzMyvNQcTMzEpzEDEzs9IcRMzMrDQHETMzK81BxMzMSnMQMTOz0hxEzMysNAcRMzMrzUHEzMxKcxAxM7PSHETMzKw0BxEzMyvNQcTMzEpzEDEzs9IcRMzMrDQHETMzK81BxMzMSnMQMTOz0hxEzMysNAcRMzMrzUHEzMxKcxAxM7PSNmx2Acx67Go1uwRmlvlKxMzMSmtYEJE0UtIdkh6RNE/SyTl9K0kzJD2W/w7J6ZJ0oaQOSQ9K2qWQ14S8/GOSJhTSd5X0UF7nQkn+impm1ocaeSWyCvhqRIwB9gROlDQGOB24PSJGA7fn9wAHAaPzayJwCaSgA5wJ7AHsDpzZGXjyMscX1hvXwPqYmVmFhgWRiFgaEffn6ReB+cC2wHhgcl5sMnBInh4PXBnJLGCwpOHAgcCMiFgREc8BM4Bxed4WETErIgK4spCXmZn1gT7pE5E0CvgAcC8wLCKW5llPA8Py9LbAosJqi3NaV+mLq6SbmVkfaXgQkbQ5cB1wSkS8UJyXryCiD8owUVK7pPbly5c3enNmZuuNhgYRSRuRAshVEXF9Tn4mN0WR/y7L6UuAkYXVR+S0rtJHVEl/k4i4NCLaIqJt6NCha1cpMzNbrZF3Zwm4DJgfET8ozJoGdN5hNQG4sZB+dL5La0/g+dzsNR04QNKQ3KF+ADA9z3tB0p55W0cX8jIzsz7QyB8b7g18FnhI0tyc9nXgPGCqpGOBhcDhed6twMFAB/Ay8DmAiFgh6TvAnLzc2RGxIk9/CbgC2AT4ZX6ZrTt68sPIoxresmvW6xoWRCLiHqDWf9B+VZYP4MQaeU0CJlVJbwfesxbFNDOzteBfrJuZWWkOImZmVpqDiJmZleYgYmZmpTmImJlZaQ4iZmZWmoOImZmV5iBiZmalOYiYmVlpDiJmZlaag4iZmZXmIGJmZqXVFUQk7V1PmpmZrV/qvRL57zrTzMxsPdLlUPCS9gI+CAyV9K+FWVsAAxpZMDMzW/d19zyRgcDmebm3FNJfAA5tVKHMzKw1dBlEIuIu4C5JV0TEwj4qk5mZtYh6n2w4SNKlwKjiOhGxbyMKZWZmraHeIPK/wI+AnwCvN644ZmbWSuoNIqsi4pKGlsTMzFpOvbf43iTpS5KGS9qq89XQkpmZ2Tqv3iuRCfnv1wppAbyjd4tjZmatpK4gEhHbN7ogZmbWeuoKIpKOrpYeEVf2bnHMzKyV1NuctVthemNgP+B+wEHEzGw9Vm9z1peL7yUNBqY0pERmZtYyyg4F/xegy34SSZMkLZP0cCHtLElLJM3Nr4ML886Q1CHpUUkHFtLH5bQOSacX0reXdG9Ov1bSwJJ1MTOzkuodCv4mSdPy6xbgUeCGbla7AhhXJf2HEbFzft2a8x8DHAHslNe5WNIASQOAi4CDgDHAkXlZgO/mvHYEngOOracuZmbWe+rtE/l+YXoVsDAiFne1QkTMlDSqzvzHA1Mi4lXgSUkdwO55XkdEPAEgaQowXtJ8YF/gqLzMZOAswD+INDPrQ3VdieSBGP9AGsl3CPDaWmzzJEkP5uauITltW2BRYZnFOa1W+luBlRGxqiK9KkkTJbVLal++fPlaFN3MzIrqbc46HJgNHAYcDtwrqcxQ8JcAOwA7A0uB80vk0WMRcWlEtEVE29ChQ/tik2Zm64V6m7O+AewWEcsAJA0Ffg38vCcbi4hnOqcl/Ri4Ob9dAowsLDoip1Ej/VlgsKQN89VIcXkzM+sj9d6dtUFnAMme7cG6q0kaXnj7SaDzzq1pwBGSBknaHhhNuvKZA4zOd2INJHW+T4uIAO5gzYOxJgA39rQ8Zma2duq9EvmVpOnANfn9/wFu7WoFSdcAY4GtJS0GzgTGStqZNO7WAuAEgIiYJ2kq8Aip4/7EiHg953MSMJ30ON5JETEvb+I0YIqkc4AHgMvqrIuZmfUSpS/1NWZKOwLDIuK3kj4F7JNnrQSuiojH+6CMvaqtrS3a29ubXQxbG1er2SVojKNq/y+aNZuk+yKirTK9uyuRC4AzACLieuD6nNl787xP9HI5zcyshXTXrzEsIh6qTMxpoxpSIjMzaxndBZHBXczbpDcLYmZmrae7INIu6fjKREnHAfc1pkhmZtYquusTOQW4QdKnWRM02oCBpFt0zcxsPdZlEMk/DvygpI8A78nJt0TEbxpeMjMzW+fV+zyRO0g/7jMzM1ut7PNEzMzMHETMzKw8BxEzMyvNQcTMzEqrdwBGs8bqr+NhmfVzvhIxM7PSfCVitq7oydWYR/y1dYSvRMzMrDQHETMzK81BxMzMSnMQMTOz0hxEzMysNAcRMzMrzUHEzMxKcxAxM7PSHETMzKw0BxEzMyvNQcTMzEpzEDEzs9IaFkQkTZK0TNLDhbStJM2Q9Fj+OySnS9KFkjokPShpl8I6E/Lyj0maUEjfVdJDeZ0LJXkscTOzPtbIK5ErgHEVaacDt0fEaOD2/B7gIGB0fk0ELoEUdIAzgT2A3YEzOwNPXub4wnqV2zIzswZrWBCJiJnAiork8cDkPD0ZOKSQfmUks4DBkoYDBwIzImJFRDwHzADG5XlbRMSsiAjgykJeZmbWR/q6T2RYRCzN008Dw/L0tsCiwnKLc1pX6YurpFclaaKkdknty5cvX7samJnZak3rWM9XEH3yZJ2IuDQi2iKibejQoX2xSTOz9UJfB5FnclMU+e+ynL4EGFlYbkRO6yp9RJV0MzPrQ30dRKYBnXdYTQBuLKQfne/S2hN4Pjd7TQcOkDQkd6gfAEzP816QtGe+K+voQl5mZtZHGvaMdUnXAGOBrSUtJt1ldR4wVdKxwELg8Lz4rcDBQAfwMvA5gIhYIek7wJy83NkR0dlZ/yXSHWCbAL/MLzMz60MNCyIRcWSNWftVWTaAE2vkMwmYVCW9HXjP2pTRzMzWjn+xbmZmpTmImJlZaQ4iZmZWmoOImZmV5iBiZmalOYiYmVlpDiJmZlaag4iZmZXmIGJmZqU5iJiZWWkOImZmVpqDiJmZleYgYmZmpTmImJlZaQ4iZmZWWsOeJ2JmDXS16l/2qGhcOWy95ysRMzMrzUHEzMxKcxAxM7PSHETMzKw0d6xb4/Sk89fMWpKvRMzMrDQHETMzK81BxMzMSnMQMTOz0poSRCQtkPSQpLmS2nPaVpJmSHos/x2S0yXpQkkdkh6UtEshnwl5+cckTWhGXczM1mfNvBL5SETsHBFt+f3pwO0RMRq4Pb8HOAgYnV8TgUsgBR3gTGAPYHfgzM7AY2ZmfWNdas4aD0zO05OBQwrpV0YyCxgsaThwIDAjIlZExHPADGBcXxfazGx91qwgEsBtku6TNDGnDYuIpXn6aWBYnt4WWFRYd3FOq5X+JpImSmqX1L58+fLeqoOZ2XqvWT823Ccilkh6GzBD0h+KMyMiJPXa0KMRcSlwKUBbW5uHNDUz6yVNuRKJiCX57zLgBlKfxjO5mYr8d1lefAkwsrD6iJxWK93MzPpInwcRSZtJekvnNHAA8DAwDei8w2oCcGOengYcne/S2hN4Pjd7TQcOkDQkd6gfkNPMzKyPNKM5axhwg6TO7V8dEb+SNAeYKulYYCFweF7+VuBgoAN4GfgcQESskPQdYE5e7uyIWNF31TAzsz4PIhHxBPD+KunPAvtVSQ/gxBp5TQIm9XYZzcysPuvSLb5mZtZiHETMzKw0BxEzMyvNQcTMzEpzEDEzs9IcRMzMrDQ/Y92sv+vJs+6P8qhA1jO+EjEzs9IcRMzMrDQHETMzK81BxMzMSnMQMTOz0hxEzMysNAcRMzMrzb8TsZ7pyW8OzKzf85WImZmV5iBiZmalOYiYmVlp7hMxszU8zpb1kK9EzMysNAcRMzMrzUHEzMxKcxAxM7PSHETMzKw0351l/hW6mZXmIGJm5dT75cO3AvdrLd+cJWmcpEcldUg6vdnlMTNbn7T0lYikAcBFwP7AYmCOpGkR8UhzS7YOcBOVrSv8A8Z+raWDCLA70BERTwBImgKMB/pvEHFwsP7MAafltHoQ2RZYVHi/GNijciFJE4GJ+e1Lkh4tub2tgT+XXHdd1l/rBf23bq7Xp1vuC1WrH7PtqiW2ehCpS0RcCly6tvlIao+Itl4o0jqlv9YL+m/dXK/W01/r1uod60uAkYX3I3KamZn1gVYPInOA0ZK2lzQQOAKY1uQymZmtN1q6OSsiVkk6CZgODAAmRcS8Bm5yrZvE1lH9tV7Qf+vmerWeflk3RfgOBzMzK6fVm7PMzKyJHETMzKw0B5E69OehVSQtkPSQpLmS2ptdnrIkTZK0TNLDhbStJM2Q9Fj+O6SZZSyrRt3OkrQkH7e5kg5uZhnLkDRS0h2SHpE0T9LJOb2lj1sX9Wr5Y1aN+0S6kYdW+SOFoVWAI/vL0CqSFgBtEdHKP4JC0j8BLwFXRsR7ctp/Aisi4rwc/IdExGnNLGcZNep2FvBSRHy/mWVbG5KGA8Mj4n5JbwHuAw4BjqGFj1sX9TqcFj9m1fhKpHurh1aJiNeAzqFVbB0SETOBFRXJ44HJeXoy6R+55dSoW8uLiKURcX+efhGYTxqFoqWPWxf16pccRLpXbWiV/nRCBHCbpPvy8DD9ybCIWJqnnwaGNbMwDXCSpAdzc1dLNflUkjQK+ABwL/3ouFXUC/rRMevkIGL7RMQuwEHAibnppN+J1G7bn9puLwF2AHYGlgLnN7c45UnaHLgOOCUiXijOa+XjVqVe/eaYFTmIdK9fD60SEUvy32XADaTmu/7imdw+3dlOvazJ5ek1EfFMRLweEW8AP6ZFj5ukjUgftFdFxPU5ueWPW7V69ZdjVslBpHv9dmgVSZvljj8kbQYcADzc9VotZRowIU9PAG5sYll6VeeHbPZJWvC4SRJwGTA/In5QmNXSx61WvfrDMavGd2fVId+KdwFrhlY5t8lF6hWS3kG6+oA0BM7VrVo3SdcAY0nDbT8DnAn8ApgKvB1YCBweES3XQV2jbmNJzSIBLABOKPQjtARJ+wB3Aw8Bb+Tkr5P6D1r2uHVRryNp8WNWjYOImZmV5uYsMzMrzUHEzMxKcxAxM7PSHETMzKw0BxEzMyvNQcQaSlJIOr/w/tQ8eGBv5H2FpEN7I69utnOYpPmS7mj0tvL2zpJ0al9sq8q275TU1oB8V9epeNwk/UTSmN7envUdBxFrtFeBT0nautkFKZLUk0dDHwscHxEfaUA5JKlp/4d5lOqmiYjj+suI2OsrBxFrtFWkZ0v/S+WMyisJSS/lv2Ml3SXpRklPSDpP0qclzc7PPtmhkM1HJbVL+qOkj+f1B0j6nqQ5ebC7Ewr53i1pGvCmDy5JR+b8H5b03Zz2LWAf4DJJ36tY/iJJ/5ynb5A0KU9/XtK5efpfc34PSzolp41Sej7NlaRfLY+U9I1ch3uAdxW28RWl51I8KGlKlTIfk/fTnUrP3zizMO8zeZ/NlfQ/nQFD0kuSzpf0e2CvKsfssLzeHyV9qJt9urmk2yXdn/fd+ML2q9apovyrr3xyuc6V9HtJsyQNy+lDJV2Xtz1H0t7V8rImiQi//GrYi/QcjC1Iv9DdEjgVOCvPuwI4tLhs/jsWWAkMBwaRxir7dp53MnBBYf1fkb4MjSaNsLwxMBH4Zl5mENAObJ/z/QuwfZVybgP8CRhK+vX+b4BD8rw7Sc9cqVznCOB7eXo2MCtPXw4cCOxK+tXyZsDmwDzSiK6jSL9k3jMv37ncpnlfdQCn5nlPAYPy9OAqZTiGNJjfW4FNSEGpDfhH4CZgo7zcxcDReTpIvwKvdrzuBM7P0wcDv87TtfbphsAWOX3rXHZ1U6fVx724b3O5PpGn/7OwvatJA4VC+hX7/Gaf136tefXkkt6slIh4IX/r/grw1zpXmxN5SAhJjwO35fSHgGKz0tRIA9o9JukJ4N2kMcDeV7jK2ZIUZF4DZkfEk1W2txtwZ0Qsz9u8Cvgn0tAptdwNnJLb9B8BhiiNj7RXruvngRsi4i85z+uBD5HGhloYEbNyPh/Ky72clyuOzfYgcJWkX3RRlhkR8WxhG/uQrgB3BeZIghRgOgcyfJ00OGAtnQMh3kcKeFB7ny4G/l1p9Oc3SI9JGNZNnWp5Dbi5sO398/RHgTG5HgBbSNo8Il6qI09rMAcR6ysXAPeTvqV3WkVuUs39AgML814tTL9ReP8Gf0qXnQAAAAIPSURBVH/eVo7bE6Rvwl+OiOnFGZLGkq5EekVELJE0GBgHzAS2Ys3T614sfOhVU285PkYKZp8AviHpvRGxqrIoVd4LmBwRZ1TJ85WIeL2LbXbu69dZs69r7dNjSFdvu0bE35SelLlx11Wq6W+RLzcqtr0B6artlZL5WgO5T8T6RKQB9KaSOqk7LSB9Wwb4Z2CjElkfJmmD3E/yDuBRYDrwRaXhuJH0TqVRirsyG/iwpK1z38GRwF11bH8WcAopiNxNaq67O8+7GzhE0qZ5+58szCuamZfbRGlU5U/kcm8AjIyIO4DTSN/+N6+y/v5KzyXfhPQUwN8CtwOHSnpbzmsrSdvVUZ9aau3TLYFlOYB8BOjcRtU6lXQb8OXON5J2Xou8rJf5SsT60vnASYX3PwZuzB28v6LcVcKfSAFgC+ALEfGKpJ+QmmHuV7ocWE43j1iNiKVKz/O+g/St+5aIqGcI8ruBAyKiQ9JC0tXI3TnP+yVdkcsH8JOIeEDpaXfFbd8v6Vrg96Qmpzl51gDgZ5K2zGW6MCJWVinDbFLz1AjgZxHRDiDpm6SnVm4A/A04kTQqbhm19ulVwE2SHiL1k/yhmzqV8RXgIkkPkj6zZgJfWIv8rBd5FF+zFpabk9oi4qTuljVrBDdnmZlZab4SMTOz0nwlYmZmpTmImJlZaQ4iZmZWmoOImZmV5iBiZmal/X+ItCvPfTA/HwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAqGCO_dhZ6L",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Number of distinct words appear in the training data: \n",
        "Excluding the `<bos>` and `<eos>` tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JWTBXRjhZ6M",
        "colab_type": "code",
        "outputId": "66fdde78-46c1-4a2f-8396-2a80023aa8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "train_words = np.concatenate([np.array(i[1:-1]) for i in train_data.title])\n",
        "c = Counter(train_words)\n",
        "total_distinct = len(c.keys())\n",
        "print(f\"Number of distinct words: {total_distinct}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of distinct words: 51298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIG7GPT6hZ6O",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The distribution of *words* will have a long tail, meaning that there are some words\n",
        "that will appear very often, and many words that will appear infrequently.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM4I7GirhZ6P",
        "colab_type": "code",
        "outputId": "ea8787dd-4e26-48fb-e243-303787934fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Extra info just for better understanding of the data\n",
        "from collections import Counter\n",
        "train_words = np.concatenate([np.array(i[1:-1]) for i in train_data.title])\n",
        "c = Counter(train_words)\n",
        "print(\"10 Most common words\")\n",
        "for entry in c.most_common(10):\n",
        "\tprint(\" Word: {0:3} Count: {1}\".format(entry[0], entry[1]))\n",
        "print('\\n','--'*25)\n",
        "\n",
        "#Part (c):\n",
        "counts = np.array(list(c.most_common()))\n",
        "print(\"\\nNumber of words that appear exactly once: \",sum(counts[:,1]=='1'))\n",
        "print(\"\\nNumber of words that appear exactly twice:\", sum(counts[:,1]=='2'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 Most common words\n",
            " Word: to  Count: 58452\n",
            " Word: ,   Count: 43088\n",
            " Word: in  Count: 38538\n",
            " Word: 's  Count: 34580\n",
            " Word: _num_ Count: 31340\n",
            " Word: :   Count: 28052\n",
            " Word: on  Count: 24794\n",
            " Word: of  Count: 22895\n",
            " Word: for Count: 22163\n",
            " Word: u.s. Count: 19320\n",
            "\n",
            " --------------------------------------------------\n",
            "\n",
            "Number of words that appear exactly once:  19854\n",
            "\n",
            "Number of words that appear exactly twice: 7193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dNRE4_6hZ6R",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaivKwhbhZ6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the <unk> tag instead of learning embeddings for rare words allows \n",
        "# me to embed unseen words. For example, if I come across a word in the validation\n",
        "# set that I didn't see in the training set, then I still have an embedding \n",
        "# for the word. Thus, I would rather replace infrequent words with <unk> tags\n",
        "# so I have a better way of dealing with both, words that appear out of \n",
        "# vocabulary and infrequently.\n",
        "# In this way, the model will learn to handle all unseen words and words for \n",
        "# which there are not enough training points. The model will be able to leverage\n",
        "# the context of the <unk> words to learn how to handle this case, and make \n",
        "# good predictions even in the validation and test sets."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5HEnhFfhZ6X",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "I will only model the top 9995 words in the training set, excluding the tags\n",
        "`<bos>`, `<eos>`, and other possible tags\n",
        "(including those, I will have a vocabulary size of exactly 10000 tokens).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foMv-_CVhZ6X",
        "colab_type": "code",
        "outputId": "65276267-b33d-4a4f-c54a-62a323edbaaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_words = np.concatenate([np.array(i[1:-1]) for i in train_data.title])\n",
        "c = Counter(train_words)\n",
        "\n",
        "total_occ = sum(c.values()) # total word occurrences\n",
        "top_occ = 0 # top word occurrences\n",
        "for entry in c.most_common(9995): top_occ += entry[1]\n",
        "print(f\"Word occurrences that will be supported: {(100 * (top_occ / total_occ)):.2f}%\")\n",
        "print(f\"Word occurrences that will be set to the <unk> tag: {(100 * (1 - top_occ / total_occ)):.2f}%\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word occurrences that will be supported: 93.98%\n",
            "Word occurrences that will be set to the <unk> tag: 6.02%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-gQLNOFhZ6a",
        "colab_type": "text"
      },
      "source": [
        "Our `torchtext` package will help us keep track of our list of unique words, known\n",
        "as a **vocabulary**. A vocabulary also assigns a unique integer index to each word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrrEDsUahZ6a",
        "colab_type": "code",
        "outputId": "68ef935c-abd0-43c2-f7e9-f7ac903e4565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Building the vocabulary based on the training data. The vocabulary\n",
        "# can have at most 9997 words (9995 words + the <bos> and <eos> token)\n",
        "text_field.build_vocab(train_data, max_size=9997)\n",
        "\n",
        "# This vocabulary object will be helpful\n",
        "vocab = text_field.vocab\n",
        "print(vocab.stoi[\"hello\"]) # for instances, I can convert from string to (unique) index\n",
        "print(vocab.itos[10])      # ... and from word index to string\n",
        "\n",
        "# The size of my vocabulary is actually 10000\n",
        "vocab_size = len(text_field.vocab.stoi)\n",
        "print(vocab_size) # should be 10000\n",
        "\n",
        "# The reason is that torchtext adds two more tokens for us:\n",
        "print(vocab.itos[0]) # <unk> represents an unknown word not in my vocabulary\n",
        "print(vocab.itos[1]) # <pad> will be used to pad short sequences for batching"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "on\n",
            "10000\n",
            "<unk>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuccFKYphZ6d",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Here is a diagram showing my desired architecture:\n",
        "\n",
        "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p4model.png\" width=\"95%\" />\n",
        "\n",
        "There are two main components to the model: the **encoder** and the **decoder**.\n",
        "As always with neural networks, I'll first describe how to make\n",
        "**predictions** with of these components. Let's get started:\n",
        "\n",
        "The **encoder** will take a sequence of words (a headline) as *input*, and produce an\n",
        "embedding (a vector) that represents the entire headline. In the diagram above,\n",
        "the vector ${\\bf h}^{(7)}$ is the vector embedding containing information about \n",
        "the entire headline.\n",
        "\n",
        "The **decoder** will take an embedding (in the diagram, the vector ${\\bf h}^{(7)}$) as input,\n",
        "and uses a separate RNN to **generate a sequence of words**. To generate a sequence of words,\n",
        "the decoder needs to do the following:\n",
        "\n",
        "1) Determine the previous word that was generated. This previous word will act as ${\\bf x}^{(t)}$\n",
        "   to our RNN, and will be used to update the hidden state ${\\bf m}^{(t)}$. Since each of our\n",
        "   sequences begin with the `<bos>` token, I'll set ${\\bf x}^{(1)}$ to be the `<bos>` token.\n",
        "2) Compute the updates to the hidden state ${\\bf m}^{(t)}$ based on the previous hidden state\n",
        "   ${\\bf m}^{(t-1)}$ and ${\\bf x}^{(t)}$. Intuitively, this hidden state vector ${\\bf m}^{(t)}$\n",
        "   is a representation of *all the words I still need to generate*.\n",
        "3) I'll use a fully-connected layer to take a hidden state ${\\bf m}^{(t)}$, and determine\n",
        "   *what the next word should be*. This fully-connected layer solves a *classification problem*,\n",
        "   since I am trying to choose a word out of $K=10000$ distinct words. As in a classification\n",
        "   problem, the fully-connected neural network will compute a *probability distribution* over\n",
        "   these 10,000 words. In the diagram, I am using ${\\bf z}^{(t)}$ to represent the logits,\n",
        "   or the pre-softmax activation values representing the probability distribution.\n",
        "4) I will need to *sample* an actual word from this probability distribution ${\\bf z}^{(t)}$.\n",
        "   I can do this in a number of ways.\n",
        "5) This word I choose will become the next input ${\\bf x}^{(t+1)}$ to our RNN, which is used\n",
        "   to update our hidden state ${\\bf m}^{(t+1)}$---i.e. to determine what are the remaining\n",
        "   words to be generated.\n",
        "\n",
        "I can repeat this process until I see an `<eos>` token generated, or until the generated\n",
        "sequence becomes too long.\n",
        "\n",
        "Unfortunately, I can't *train* this autoencoder in the way I just described. That is,\n",
        "I can't just compare our generated sequence with our ground-truth sequence, and get\n",
        "gradients. Both sequences are **discrete** entities, so I won't be able to compute\n",
        "gradients at all! In particular, **sampling is a discrete process**, and so I won't be\n",
        "able to back-propagate through any kind of sampling that I do.\n",
        "\n",
        "Fortunately, there is one very well-established solution called\n",
        "**teacher forcing** which I can use for training:\n",
        "instead of *sampling* the next word based on ${\\bf z}^{(t)}$, I will forgo sampling,\n",
        "and use the **ground truth** ${\\bf x}^{(t)}$ in the next step.\n",
        "\n",
        "Here is a diagram showing how I can use **teacher forcing** to train our model:\n",
        "\n",
        "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p4model_tf.png\" width=\"95%\" />\n",
        "\n",
        "I will use the RNN generator to compute the logits\n",
        "${\\bf z}^{(1)},{\\bf z}^{(2)},  \\cdots {\\bf z}^{(T)}$. These distributions\n",
        "can be compared to the ground-truth words using the cross-entropy loss.\n",
        "The loss function for this model will be the sum of the losses across each $t$.\n",
        "(This is similar to what I did in a pixel-wise prediction problem.)\n",
        "\n",
        "I'll train the encoder and decoder model simultaneously. There are several components\n",
        "to our model that contain tunable weights:\n",
        "\n",
        "- The word embedding that maps a word to a vector representation.\n",
        "  The word embedding component is represented with blue arrows in the diagram.\n",
        "- The encoder RNN (which will use Gated Recurrent Units) that computes the\n",
        "  embedding over the entire headline. The encoder RNN \n",
        "  is represented with black arrows in the diagram.\n",
        "- The decoder RNN (which will also use Gated Recurrent Units) that computes\n",
        "  hidden states, which are vectors representing what words are to be generated.\n",
        "  The decoder RNN is represented with gray arrows in the diagram.\n",
        "- The **projection MLP** (a fully-connected layer) that computes\n",
        "  a distribution over the next word to generate, given a decoder RNN hidden\n",
        "  state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oujKpi3ThZ6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        \"\"\"\n",
        "        A text autoencoder. The parameters \n",
        "            - vocab_size: number of unique words/tokens in the vocabulary\n",
        "            - emb_size: size of the word embeddings $x^{(t)}$\n",
        "            - hidden_size: size of the hidden states in both the\n",
        "                           encoder RNN ($h^{(t)}$) and the\n",
        "                           decoder RNN ($m^{(t)}$)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                  embedding_dim=emb_size)  \n",
        "        self.encoder_rnn = nn.GRU(input_size=emb_size, \n",
        "                                  hidden_size=hidden_size,\n",
        "                                  batch_first=True)\n",
        "        self.decoder_rnn = nn.GRU(input_size=emb_size,\n",
        "                                  hidden_size=hidden_size,\n",
        "                                  batch_first=True)\n",
        "        self.proj = nn.Linear(in_features=hidden_size,\n",
        "                              out_features=vocab_size)\n",
        "\n",
        "    def encode(self, inp):\n",
        "        \"\"\"\n",
        "        Computes the encoder output given a sequence of words.\n",
        "        \"\"\"\n",
        "        emb = self.embed(inp)\n",
        "        out, last_hidden = self.encoder_rnn(emb)\n",
        "        return last_hidden\n",
        "\n",
        "    def decode(self, inp, hidden=None):\n",
        "        \"\"\"\n",
        "        Computes the decoder output given a sequence of words, and\n",
        "        (optionally) an initial hidden state.\n",
        "        \"\"\"\n",
        "        emb = self.embed(inp)\n",
        "        out, last_hidden = self.decoder_rnn(emb, hidden)\n",
        "        out_seq = self.proj(out)\n",
        "        return out_seq, last_hidden\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"\n",
        "        Compute both the encoder and decoder forward pass\n",
        "        given an integer input sequence inp with shape [batch_size, seq_length],\n",
        "        with inp[a,b] representing the (index in our vocabulary of) the b-th word\n",
        "        of the a-th training example.\n",
        "\n",
        "        This function should return the logits $z^{(t)}$ in a tensor of shape\n",
        "        [batch_size, seq_length - 1, vocab_size], computed using *teaching forcing*.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        last_hidden_encoded = self.encode(inp)\n",
        "        output, hidden = self.decode(inp[:,:-1], last_hidden_encoded)\n",
        "        return output, hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orT4cOnwhZ6h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "To check that my model is set up correctly, I'll train our AutoEncoder\n",
        "neural network for at least 300 iterations to memorize this sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7mmFLtDhZ6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline = train_data[42].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).long().unsqueeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Y9CKnIhZ6j",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mh5480ehZ6k",
        "colab_type": "code",
        "outputId": "324a42dc-e3f1-48fc-a64f-7b84897586d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model = AutoEncoder(vocab_size, 128, 128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for it in range(300):\n",
        "    optimizer.zero_grad()\n",
        "    output, hidden = model(input_seq)\n",
        "    target = input_seq[:,1:]\n",
        "    loss = criterion(output.reshape(-1, vocab_size),\n",
        "                 target.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (it+1) % 50 == 0:\n",
        "        print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 50] Loss 0.088019\n",
            "[Iter 100] Loss 0.025627\n",
            "[Iter 150] Loss 0.016161\n",
            "[Iter 200] Loss 0.011296\n",
            "[Iter 250] Loss 0.008410\n",
            "[Iter 300] Loss 0.006544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcZYTF9jhZ6n",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Note that I am sampling from a multi-nomial distribution described\n",
        "by the logits $z^{(t)}$. For example, if our distribution is [80%, 20%]\n",
        "over a vocabulary of two words, then I will choose the first word\n",
        "with 80% probability and the second word with 20% probability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjDTLU6ChZ6n",
        "colab_type": "code",
        "outputId": "0aadeac0-d58e-448e-b64b-33cdbfb4d744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def sample_sequence(model, hidden, max_len=20, temperature=1):\n",
        "    \"\"\"\n",
        "    Return a sequence generated from the model's decoder\n",
        "        - model: an instance of the AutoEncoder model\n",
        "        - hidden: a hidden state (e.g. computed by the encoder)\n",
        "        - max_len: the maximum length of the generated sequence\n",
        "        - temperature: described below\n",
        "    \"\"\"\n",
        "    # I'll store our generated sequence here\n",
        "    generated_sequence = []\n",
        "    # Set input to the <BOS> token\n",
        "    inp = torch.Tensor([text_field.vocab.stoi[\"<bos>\"]]).long()\n",
        "    for p in range(max_len):\n",
        "        # compute the output and next hidden unit\n",
        "        output, hidden = model.decode(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted word to string and use as next input\n",
        "        word = text_field.vocab.itos[top_i]\n",
        "        # Break early if I reach <eos>\n",
        "        if word == \"<eos>\":\n",
        "            break\n",
        "        generated_sequence.append(word)\n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        "\n",
        "for i in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  seq = sample_sequence(model, hidden)\n",
        "  print(f'Iteration: {i}; Generated Sequence: {seq}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0; Generated Sequence: ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iteration: 1; Generated Sequence: ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iteration: 2; Generated Sequence: ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iteration: 3; Generated Sequence: ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iteration: 4; Generated Sequence: ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOS0oCWQhZ6t",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The multi-nomial distribution can be manipulated using the `temperature`\n",
        "setting. This setting can be used to make the distribution \"flatter\" (e.g.\n",
        "more likely to generate different words) or \"peakier\" (e.g. less likely\n",
        "to generate different words).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykloMqSIhZ6t",
        "colab_type": "code",
        "outputId": "f9d1c760-e658-4d30-aed3-e66b8e473ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "\"\"\"\n",
        "If I use larger temperature settings it makes all actions more equally likely,\n",
        "and thus gives us \"more random\" outputs. However, lower values (less than 1)\n",
        "makes high probabilities contribute more. As the temperature values move towards\n",
        "zero I get only the most likely outputs.\n",
        "\"\"\"\n",
        "for temp in [0.5,1.5, 2, 4]:\n",
        "  for i in range(5):\n",
        "    hidden = model.encode(input_seq)\n",
        "    seq = sample_sequence(model, hidden, temperature=temp)\n",
        "    print(f'Iter: {i}; Temp: {temp}; Generated Sequence:\\n   {seq}')\n",
        "  print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0; Temp: 0.5; Generated Sequence:\n",
            "   ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iter: 1; Temp: 0.5; Generated Sequence:\n",
            "   ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iter: 2; Temp: 0.5; Generated Sequence:\n",
            "   ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iter: 3; Temp: 0.5; Generated Sequence:\n",
            "   ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iter: 4; Temp: 0.5; Generated Sequence:\n",
            "   ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "\n",
            "\n",
            "Iter: 0; Temp: 1.5; Generated Sequence:\n",
            "   ['constellation', 'sox', 'chief', 'masks', 'professor', 'boss', 'woods', 'insolvency', 'quarantined', 'obama', 'valentine', 'ill-fated', 'wheels', 'acquiring', 'chief']\n",
            "Iter: 1; Temp: 1.5; Generated Sequence:\n",
            "   ['zambian', 'president', 'swears', 'transmission', 'ousted', 'magna', 'first', 'liable', 'boeing', 'cincinnati', 'year-on-year', 'swears', 'in', 'new', 'escalating', 'future', 'exposes', 'lags', 'brussels', 'army']\n",
            "Iter: 2; Temp: 1.5; Generated Sequence:\n",
            "   ['president', 'guatemala', 'in', 'new', 'von', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Iter: 3; Temp: 1.5; Generated Sequence:\n",
            "   ['zambian', 'president', 'swears', 'spacecraft', 'chief', 'views', 'scales', 'nickel', 'tons']\n",
            "Iter: 4; Temp: 1.5; Generated Sequence:\n",
            "   ['zambian', 'swears', 'in', 'new', 'divisions', 'browns', 'ms', 'ban', 'new', 'army', 'chief']\n",
            "\n",
            "\n",
            "Iter: 0; Temp: 2; Generated Sequence:\n",
            "   ['nutrition', 'issue', 'warm', 'towers', 'uses', 'argues', 'politically', 'computers', 'witnesses', 'retire', 'chief', 'fully', 'charges', 'twice', 'shy', 'zambian', 'coleman', 'prudent', 'wireless', 'pratt']\n",
            "Iter: 1; Temp: 2; Generated Sequence:\n",
            "   ['indivior', 'sanofi', 'protesters', 'hindu', 'vestager', 'esports', 'kavanaugh', 'daughter', 'defiant', 'cognizant', 'describe', 'launch', 'rubio', 'zambian', 'president', 'sinai', 'pop', 'settlement', 'army', 'absence']\n",
            "Iter: 2; Temp: 2; Generated Sequence:\n",
            "   ['denmark', 'behind', 'uefa', 'reflect', 'americas', 'rage', 'finding', 'determined', 'breakthrough', 'erase', '_num_-hedge', 'boosts', 'russians', 'machinery', 'pump', 'commuter', 'americas', 'senior', 'greenpeace', 'kurdish']\n",
            "Iter: 3; Temp: 2; Generated Sequence:\n",
            "   ['eats', 'concedes', 'slate', 'bnp', 'decades', 'merchants', 'heightened', 'put', 'nadal', '_num_-chinese', '_num_-euro', 'identity', 'looking', 'o', 'shifts', 'lawrence', 'kamala', 'fares', 'chief', '_num_-vodafone']\n",
            "Iter: 4; Temp: 2; Generated Sequence:\n",
            "   ['steel', 'globally', 'poses', 'regrettable', 'sweetens', 'restores', 'bout', 'jesus', 'treaty', '_num_-thyssenkrupp', 'chief', 'student', 'backers', 'drugmakers', 'iran-backed', 'pass', 'nasa', 'h', 'importers', 'stalled']\n",
            "\n",
            "\n",
            "Iter: 0; Temp: 4; Generated Sequence:\n",
            "   ['ease', 'slowed', 'exclusion', 'klm', 'asia', 'alleging', 'become', 'julius', 'spacex', 'hidden', 'sue', 'gp', 'eqt', 'bite', 'stave', 'retreat', 'congressman', 'wins', 'clean-up', 'choke']\n",
            "Iter: 1; Temp: 4; Generated Sequence:\n",
            "   ['slashing', '_num_-cn', 'magnitude', 'coach', 'big', 'fortunes', 'sworn', 'muguruza', 'promote', 'sanction', 'box', 'blaming', 'nbc', 'objects', 'spoke', 'feed', 'climate-change', 'shipper', 'lands', 'vertex']\n",
            "Iter: 2; Temp: 4; Generated Sequence:\n",
            "   ['mainland', 'bringing', 'nothing', 'arteta', 'victoria', 'rounds', 'spray', 'elizabeth', 'any', 'decisive', 'starbucks', 'lets', 'racing', 'failures', 'cocoa', 'glass', 'promote', 'undecided', 'ticket', 'half']\n",
            "Iter: 3; Temp: 4; Generated Sequence:\n",
            "   ['lung', 'venezuelans', 'torrey', 'restarting', 'stuns', 'weakest', 'lowest', 'alleging', 'office', \"'s\", 'tribunal', 'clouds', 'adam', 'buffett', 'piles', 'brewer', 'except', 'alitalia', 'awarded', 'abandon']\n",
            "Iter: 4; Temp: 4; Generated Sequence:\n",
            "   ['carmakers', '_num_-bristol-myers', 'proposed', 'costa', 'fruit', 'dementia', 'kim', 'rains', 'arrested', 'not', 'inches', 'models', 'mugabe', 'druzhba', 'effort', \"'ve\", 'reputation', 'write', 'fuelling', 'term']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFUdT2F3hZ6v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "It turns out that getting good results from a text auto-encoder is very difficult,\n",
        "and that it is very easy for our model to **overfit**.\n",
        "I will prevent overfitting using **data augmentation**.\n",
        "\n",
        "The idea behind data augmentation is to artificially increase the number of training\n",
        "examples by \"adding noise\" to the image. For example, during AlexNet training,\n",
        "the authors randomly cropped $224\\times 224$\n",
        "regions of a $256 \\times 256$ pixel image to increase the amount of training data.\n",
        "The authors also flipped the image left/right.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvRwJnOKhZ6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Note: The exact augmentation technique that should be used and its effect on\n",
        "training is highly dependent of the type of data you have.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MqjXogfhZ64",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "I will add noise to our headlines using a few different techniques:\n",
        "\n",
        "1. Shuffle the words in the headline, taking care that words don't end up too far from where they were initially\n",
        "2. Drop (remove) some words \n",
        "3. Replace some words with a blank word (a `<pad>` token)\n",
        "4. Replace some words with a random word \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn7T9gG3hZ64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_randomize(headline,\n",
        "                           drop_prob=0.1,  # probability of dropping a word\n",
        "                           blank_prob=0.1, # probability of \"blanking\" out a word\n",
        "                           sub_prob=0.1,   # probability of substituting a word with a random one\n",
        "                           shuffle_dist=3): # maximum distance to shuffle a word\n",
        "    \"\"\"\n",
        "    Add 'noise' to a headline by slightly shuffling the word order,\n",
        "    dropping some words, blanking out some words (replacing with the <pad> token)\n",
        "    and substituting some words with random ones.\n",
        "    \"\"\"\n",
        "    headline = [vocab.stoi[w] for w in headline.split()]\n",
        "    n = len(headline)\n",
        "    # shuffle\n",
        "    headline = [headline[i] for i in get_shuffle_index(n, shuffle_dist)]\n",
        "\n",
        "    new_headline = [vocab.stoi['<bos>']]\n",
        "    for w in headline:\n",
        "        if random.random() < drop_prob:\n",
        "            # drop the word\n",
        "            pass\n",
        "        elif random.random() < blank_prob:\n",
        "            # replace with blank word\n",
        "            new_headline.append(vocab.stoi[\"<pad>\"])\n",
        "        elif random.random() < sub_prob:\n",
        "            # substitute word with another word\n",
        "            new_headline.append(random.randint(0, vocab_size - 1))\n",
        "        else:\n",
        "            # keep the original word\n",
        "            new_headline.append(w)\n",
        "    new_headline.append(vocab.stoi['<eos>'])\n",
        "    return new_headline\n",
        "\n",
        "def get_shuffle_index(n, max_shuffle_distance):\n",
        "    \"\"\" This is a helper function used to shuffle a headline with n words,\n",
        "    where each word is moved at most max_shuffle_distance. The function does\n",
        "    the following: \n",
        "       1. start with the *unshuffled* index of each word, which\n",
        "          is just the values [0, 1, 2, ..., n]\n",
        "       2. perturb these \"index\" values by a random floating-point value between\n",
        "          [0, max_shuffle_distance]\n",
        "       3. use the sorted position of these values as our new index\n",
        "    \"\"\"\n",
        "    index = np.arange(n)\n",
        "    perturbed_index = index + np.random.rand(n) * 3\n",
        "    new_index = sorted(enumerate(perturbed_index), key=lambda x: x[1])\n",
        "    return [index for (index, pert) in new_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K4KMU98hZ67",
        "colab_type": "code",
        "outputId": "4574ced7-7d79-414a-a4bd-ea61cd6a2aae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def run_tokenize_and_randomize(number_iters, headline, randomize_values=False):\n",
        "  for i in range(number_iters):\n",
        "    if randomize_values:\n",
        "      shuff = random.randint(1,5) # random num between 2 and 5\n",
        "      # random num between 0.1 and 0.5 for the following:\n",
        "      drp = round(0.4*np.random.random_sample()+0.1, 2)\n",
        "      blnk = round(0.4*np.random.random_sample()+0.1, 2)\n",
        "      sub = round(0.4*np.random.random_sample()+0.1, 2)\n",
        "      print(f'Iter: {i}, Shuff Dist: {shuff}, P(Drop): {drp}, P(Blank): {blnk\\\n",
        "                                                              }, P(Sub): {sub}')\n",
        "      new_head_indexs = tokenize_and_randomize(headline,drp,blnk,sub,shuff)\n",
        "    else: #use defaults\n",
        "      new_head_indexs = tokenize_and_randomize(headline)\n",
        "      print(f'Iter: {i}')\n",
        "    print(f'\\tOriginal Headline: {headline}')\n",
        "    new_head = [vocab.itos[w] for w in new_head_indexs]\n",
        "    new_head = ' '.join(new_head) # to make headline more readable!\n",
        "    print(f'\\tNew Headline: {new_head}\\n')\n",
        "\n",
        "original_headline = ' '.join(train_data[22].title)\n",
        "run_tokenize_and_randomize(5, original_headline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> <bos> india cuts tax <pad> , crude iaea imports oil from asean countries <pad> <eos>\n",
            "\n",
            "Iter: 1\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> atlantia cuts tax on , refined crude palm oil imports asean countries <pad> <eos>\n",
            "\n",
            "Iter: 2\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> <bos> cuts india tax , repayment palm oil from imports asean <eos> <eos>\n",
            "\n",
            "Iter: 3\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> india <bos> cuts lyles winning <pad> refined skripal from imports asean countries <eos>\n",
            "\n",
            "Iter: 4\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> primed india cuts on tax crude palm , refined district asean from _num_-glencore <eos>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnRRXjy86LGU",
        "colab_type": "code",
        "outputId": "cbea456b-f38a-430c-f7de-76b8fe41ea00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# For testing the effects of different values in tokenize_and_randomize:\n",
        "run_tokenize_and_randomize(5, original_headline, True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Shuff Dist: 1, P(Drop): 0.4, P(Blank): 0.47, P(Sub): 0.13\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> india <pad> tax <pad> crude oil <pad> <pad> <eos>\n",
            "\n",
            "Iter: 1, Shuff Dist: 1, P(Drop): 0.23, P(Blank): 0.43, P(Sub): 0.21\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> <pad> shifting tax <pad> <pad> refined oil palm stephens <pad> countries <eos>\n",
            "\n",
            "Iter: 2, Shuff Dist: 5, P(Drop): 0.24, P(Blank): 0.32, P(Sub): 0.37\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> india <pad> <pad> tax on refined <pad> <pad> collins sabres <pad> <eos>\n",
            "\n",
            "Iter: 3, Shuff Dist: 1, P(Drop): 0.21, P(Blank): 0.28, P(Sub): 0.44\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> india <pad> cuts crude projectiles , crunch imports insurer census debates <eos>\n",
            "\n",
            "Iter: 4, Shuff Dist: 1, P(Drop): 0.19, P(Blank): 0.41, P(Sub): 0.29\n",
            "\tOriginal Headline: <bos> india cuts tax on crude , refined palm oil imports from asean countries <eos>\n",
            "\tNew Headline: <bos> iowa <bos> on <pad> encryption refined palm oil <pad> <pad> <pad> <eos>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2zcmZkGhZ6-",
        "colab_type": "code",
        "outputId": "c5a5f035-b329-48e7-80c0-faf84b5264a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "def train_autoencoder(model, batch_size=64, learning_rate=0.001, num_epochs=10):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(num_epochs):\n",
        "        # I will perform data augmentation by re-reading the input each time\n",
        "        field = torchtext.data.Field(sequential=True,\n",
        "                                     tokenize=tokenize_and_randomize, # <-- data augmentation\n",
        "                                     include_lengths=True,\n",
        "                                     batch_first=True,\n",
        "                                     use_vocab=False, \n",
        "                                     pad_token=vocab.stoi['<pad>'])\n",
        "        dataset = torchtext.data.TabularDataset(train_path, \"tsv\", [('title', field)])\n",
        "        val_dataset = torchtext.data.TabularDataset(valid_path, \"tsv\", [('title', text_field)])\n",
        "\n",
        "        # This BucketIterator will handle padding of sequences that are not of the same length\n",
        "        train_iter = torchtext.data.BucketIterator(dataset,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   sort_key=lambda x: len(x.title), # to minimize padding\n",
        "                                                   repeat=False)\n",
        "        valid_iter = torchtext.data.BucketIterator(val_dataset,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   sort_key=lambda x: len(x.title), # to minimize padding\n",
        "                                                   repeat=False)\n",
        "        model.train() # labelling model for training\n",
        "        model.cuda() # moving model from CPU to GPU\n",
        "\n",
        "        # BACKPROP on Training Data\n",
        "        for it, ((xs, lengths), _) in enumerate(train_iter):\n",
        "            xs = xs.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output, hidden = model(xs)\n",
        "            target = xs[:,1:]\n",
        "            loss = criterion(output.reshape(-1, vocab_size),\n",
        "                        target.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            xs = xs.cpu()\n",
        "            if (it+1) % 100 == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))\n",
        "        \n",
        "        # Calculating Loss for Validation Data\n",
        "        model.train(False)\n",
        "        val_loss = 0\n",
        "        val_n = 0\n",
        "        for it, ((xs, lengths), _) in enumerate(valid_iter):\n",
        "           xs = xs.cuda()\n",
        "           zs,_ = model(xs)\n",
        "           target = xs[:,1:]\n",
        "           loss = criterion(zs.reshape(-1, vocab_size),\n",
        "                        target.reshape(-1))\n",
        "           val_loss += float(loss)\n",
        "           xs = xs.cpu()\n",
        "        model.cpu()\n",
        "        print(f'[Epoch {ep}] Valid Loss {val_loss/len(valid_iter)}\\n')\n",
        "\n",
        "model = AutoEncoder(vocab_size, 128, 128)\n",
        "train_autoencoder(model, 64, 0.002, num_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 100] Loss 4.141483\n",
            "[Iter 200] Loss 4.309234\n",
            "[Iter 300] Loss 3.837526\n",
            "[Iter 400] Loss 4.149048\n",
            "[Iter 500] Loss 3.824544\n",
            "[Iter 600] Loss 4.010885\n",
            "[Iter 700] Loss 3.763700\n",
            "[Iter 800] Loss 2.745450\n",
            "[Iter 900] Loss 3.124757\n",
            "[Iter 1000] Loss 3.308589\n",
            "[Iter 1100] Loss 3.572793\n",
            "[Iter 1200] Loss 3.453383\n",
            "[Iter 1300] Loss 3.303916\n",
            "[Iter 1400] Loss 3.157914\n",
            "[Iter 1500] Loss 3.729188\n",
            "[Iter 1600] Loss 2.969416\n",
            "[Iter 1700] Loss 3.045413\n",
            "[Iter 1800] Loss 2.841511\n",
            "[Iter 1900] Loss 2.961511\n",
            "[Iter 2000] Loss 3.536849\n",
            "[Iter 2100] Loss 3.146111\n",
            "[Iter 2200] Loss 2.927870\n",
            "[Iter 2300] Loss 2.548130\n",
            "[Iter 2400] Loss 2.993264\n",
            "[Iter 2500] Loss 2.964173\n",
            "[Iter 2600] Loss 3.217925\n",
            "[Epoch 0] Valid Loss 2.9141917892750477\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHdN7fZFhZ7A",
        "colab_type": "code",
        "outputId": "05815597-2b16-46d5-8b48-acfd6ab5480c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = AutoEncoder(10000, 128, 128)\n",
        "checkpoint_path = '/content/drive/My Drive/model.pk'\n",
        "model.load_state_dict(torch.load(checkpoint_path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALgJgfUBhZ7C",
        "colab_type": "code",
        "outputId": "07d4b057-fff3-4a77-947d-efbdd2367f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "# If the temperature setting is too small then all of the generated sequences\n",
        "# would be the same/very similar. This is because I would only select the values\n",
        "# that have the highest probability of contributing to the sequence. Thus I \n",
        "# use a moderate temperature setting.\n",
        "headline = train_data[10].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).unsqueeze(0).long()\n",
        "print(f'Original Headline: {headline}\\n\\n')\n",
        "for temp in [0.7,0.9,1.5]:\n",
        "  for i in range(5):\n",
        "    hidden = model.encode(input_seq)\n",
        "    seq = sample_sequence(model, hidden, temperature=temp)\n",
        "    print(f'Iter: {i}; Temp: {temp}; Generated Sequence:\\n   {seq}')\n",
        "  print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Headline: ['<bos>', 'wall', 'street', 'rises', ',', 'limps', 'across', 'the', 'finish', 'line', 'of', 'a', 'turbulent', 'year', '<eos>']\n",
            "\n",
            "\n",
            "Iter: 0; Temp: 0.7; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'limps', 'and', 'wins', 'the', 'costs', 'baghdad', '<pad>', 'for', 'challenging']\n",
            "Iter: 1; Temp: 0.7; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'limps', 'and', 'wins', 'at', '$', '<pad>', 'highway', 'a', 'bets']\n",
            "Iter: 2; Temp: 0.7; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'limps', 'and', 'wins', 'at', 'menu', ':', 'libya', 'unknown']\n",
            "Iter: 3; Temp: 0.7; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'limps', 'and', 'wins', 'at', '$', '<pad>', 'after', 'camps', 'bets']\n",
            "Iter: 4; Temp: 0.7; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'limps', 'and', 'wins', 'at', '$', '<pad>', 'after', 'positive', 'election']\n",
            "\n",
            "\n",
            "Iter: 0; Temp: 0.9; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'after', 'the', 'losses', 'from', 'the', 'markets', 'poor', 'performing', 'year']\n",
            "Iter: 1; Temp: 0.9; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'after', 'then', 'losses', \"'s\", 'question', '<pad>', 'outside', 'summit', 'another']\n",
            "Iter: 2; Temp: 0.9; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'after', 'australia', 'back', 'clears', 'line', 'on', '<pad>', 'year', 'this']\n",
            "Iter: 3; Temp: 0.9; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'after', 'die', 'wins', 'at', 'choppy', '<pad>', 'pact', 'near', 'year']\n",
            "Iter: 4; Temp: 0.9; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'after', 'open', 'sentence', ',', 'kick', '<unk>', 'big', 'india', 'job']\n",
            "\n",
            "\n",
            "Iter: 0; Temp: 1.5; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'limps', 'refugee', '<unk>', 'marathon', 'funding', 'says', 'fire', 'september', 'competition']\n",
            "Iter: 1; Temp: 1.5; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', \"'s\", 'stones', 'silent', 'to', 'norway', '6th']\n",
            "Iter: 2; Temp: 1.5; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'limps', 'point', 'die', \"'s\", 'extradition', 'suppliers', 'after', 'september', 'in']\n",
            "Iter: 3; Temp: 1.5; Generated Sequence:\n",
            "   ['wall', 'street', 'rises', ',', 'fc', 'lawmakers', 'win', 'australia', 'on', '<pad>', 'for', 'baghdad', 'surplus']\n",
            "Iter: 4; Temp: 1.5; Generated Sequence:\n",
            "   ['wall', 'survey', 'federer', 'giannis', 'assets', 'but', 'greener', 'into', 'appeal', 'to', 'park', 'abbas', 'risks']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O368lQF1hZ7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_data = torchtext.data.TabularDataset(\n",
        "    path=valid_path,                # data file path\n",
        "    format=\"tsv\",                   # fields are separated by a tab\n",
        "    fields=[('title', text_field)]) # list of fields (I have only one)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtl5H2NHhZ7I",
        "colab_type": "code",
        "outputId": "610e733a-8ab4-4e95-ecbc-96e174bf8ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_embeddings = []\n",
        "for val_point in valid_data:\n",
        "  input_seq1 = torch.Tensor([vocab.stoi[w] for w in val_point.title]).long().unsqueeze(0)\n",
        "  encoding = model.encode(input_seq1)[0]\n",
        "  val_embeddings.append(encoding)\n",
        "val_embeddings = torch.cat(tuple(val_embeddings),0)\n",
        "print(val_embeddings.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19046, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn4QUnpphZ7N",
        "colab_type": "code",
        "outputId": "c3db482d-d8de-4b8b-a3e7-58c42fcd2e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "word_emb = val_embeddings.detach().numpy()\n",
        "norms=np.linalg.norm(word_emb, axis=1)\n",
        "word_emb_norm=(word_emb.T/norms).T\n",
        "similarities=np.matmul(word_emb_norm, word_emb_norm.T)\n",
        "def find_five_closest(index):\n",
        "  \"\"\" \n",
        "  Return a list of the five closest headlines to the headline at given index. \n",
        "  \"\"\"\n",
        "  ind = np.argsort([similarities[index]])\n",
        "  five_closest = [valid_data[i] for i in ind[0,-6:-1]] \n",
        "  five_closest.reverse()\n",
        "  return five_closest\n",
        "\n",
        "print(f'Original Headline:\\n{\" \".join(valid_data[13].title)}')\n",
        "closest = find_five_closest(13)\n",
        "print('\\n5 Closest Headlines:')\n",
        "for close_headline in closest:\n",
        "  print(f'{\" \".join(close_headline.title)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Headline:\n",
            "<bos> asia takes heart from new year gains in u.s. stock futures <eos>\n",
            "\n",
            "5 Closest Headlines:\n",
            "<bos> italy 's salvini loses aura of invincibility in emilia setback <eos>\n",
            "<bos> saudi , russia look to seal deeper output cuts with oil producers <eos>\n",
            "<bos> eu orders quarantine for staff who traveled to northern italy <eos>\n",
            "<bos> update _num_-italy 's prime minister says new government will bicker less <eos>\n",
            "<bos> portugal 's moura pays tribute to cod fishermen at milan fashion close <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qspCs6HEhZ7P",
        "colab_type": "code",
        "outputId": "417519c7-7628-48a1-aa0e-dc5d07dbc576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(f'Original Headline:\\n{\" \".join(valid_data[7727].title[1:-1])}')\n",
        "closest = find_five_closest(7727)\n",
        "print('\\n5 Closest Headlines:')\n",
        "for close_headline in closest:\n",
        "  print(f'{\" \".join(close_headline.title[1:-1])}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Headline:\n",
            "california governor signs bill for $ _num_ bln wildfire fund\n",
            "\n",
            "5 Closest Headlines:\n",
            "california lawmakers approve legislation for $ _num_ bln wildfire fund\n",
            "update _num_-california governor proposes a $ _num_ billion wildfire fund\n",
            "california governor proposes more than $ _num_ billion toward homelessness\n",
            "california governor signs gig economy labor bill into law\n",
            "u.s. group says novartis ms drug price out of line with benefit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJFVYWfBhZ7S",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "I will choose two headlines from the validation set, and find their embeddings.\n",
        "I will then **interpolate** between the two embeddings.\n",
        "\n",
        "3 points, equally spaced between the embeddings of my headlines.\n",
        "If I let $e_0$ be the embedding of the first headline and $e_4$ be\n",
        "the embedding of the second headline, the three points should be:\n",
        "\n",
        "\\begin{align*}\n",
        "e_1 &=  0.75 e_0 + 0.25 e_4 \\\\\n",
        "e_2 &=  0.50 e_0 + 0.50 e_4 \\\\\n",
        "e_3 &=  0.25 e_0 + 0.75 e_4 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "I decode each of $e_1$, $e_2$ and $e_3$ five times, with a temperature setting\n",
        "that shows some variation in the generated sequences, while generating sequences\n",
        "that makes sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSHoBDxahZ7S",
        "colab_type": "code",
        "outputId": "291582b4-958f-4e8a-ec64-69ac87a5c9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "def interpolate(index1, index2, temp): #0,33\n",
        "  headline_1 = valid_data[index1].title\n",
        "  input_seq1 = torch.Tensor([vocab.stoi[w] for w in headline_1]).long().unsqueeze(0)\n",
        "  headline_2 = valid_data[index2].title\n",
        "  input_seq2 = torch.Tensor([vocab.stoi[w] for w in headline_2]).long().unsqueeze(0)\n",
        "  x = torch.cat([input_seq1,input_seq2])\n",
        "  embedding = model.encode(x)\n",
        "  e0 = embedding[:,0,:]\n",
        "  e4 = embedding[:,1,:]\n",
        "  embedding_values = []\n",
        "  for i in range(1, 4):\n",
        "      e = (e0 * ((4-i)/4)) + (e4 * (i/4))\n",
        "      embedding_values.append(e)\n",
        "  embedding_values = torch.stack(embedding_values)\n",
        "  for i in range(1,6):\n",
        "    print('\\n')\n",
        "    print('--'*50)\n",
        "    print(f'Iter: {i}; Temp: {temp};')\n",
        "    print('1st (e0):', headline_1)\n",
        "    for embed in range(0,3):\n",
        "      seq = sample_sequence(model, embedding_values[embed,:].view(1,1,128), temperature=temp)\n",
        "      print(f'(e{embed+1}):  {seq}')\n",
        "    print('2nd (e4):', headline_2)\n",
        "    print('--'*50)\n",
        "interpolate(0,33, .79)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Iter: 1; Temp: 0.79;\n",
            "1st (e0): ['<bos>', 'n.korea', \"'s\", 'kim', 'says', 'new', 'path', 'inevitable', 'if', 'u.s.', 'demands', 'unilateral', 'action', '<eos>']\n",
            "(e1):  ['n.korea', 'situation', 'says', 'states', 'new', 'soft', 'leave', 'will', 'force', 'the', 'warned', 'dollars']\n",
            "(e2):  ['experts', 'pressing', 'bolsonaro', 'to', 'defend', 'compensation', 'not', 'tensions']\n",
            "(e3):  ['kurdish', 'syrian', 'seal', 'to', 'rocky', 'development', 'of', 'the', 'airliner', ':', 'korea', 'extremism']\n",
            "2nd (e4): ['<bos>', 'kurdish', 'fighters', 'pull', 'out', 'of', 'flashpoint', 'town', ':', 'syria', \"'s\", 'defense', 'ministry', '<eos>']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Iter: 2; Temp: 0.79;\n",
            "1st (e0): ['<bos>', 'n.korea', \"'s\", 'kim', 'says', 'new', 'path', 'inevitable', 'if', 'u.s.', 'demands', 'unilateral', 'action', '<eos>']\n",
            "(e1):  ['n.korea', 'says', 'join', 'pilot', 'confident', 'not', 'secretive', 'ready', 'venezuela', 'egypt', 'cnbc']\n",
            "(e2):  ['kurdish', 'drian', 'carrier', 'of', 'take', 'loom', 'ukrainian', 'u.s.', 'summit', 'sale', 'direction', 'tariffs']\n",
            "(e3):  ['kurdish', 'senior', 'returns', 'exports', 'drones', 'carefully', 'against', 'kashmir', 'iran', 'chief', 'data']\n",
            "2nd (e4): ['<bos>', 'kurdish', 'fighters', 'pull', 'out', 'of', 'flashpoint', 'town', ':', 'syria', \"'s\", 'defense', 'ministry', '<eos>']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Iter: 3; Temp: 0.79;\n",
            "1st (e0): ['<bos>', 'n.korea', \"'s\", 'kim', 'says', 'new', 'path', 'inevitable', 'if', 'u.s.', 'demands', 'unilateral', 'action', '<eos>']\n",
            "(e1):  ['theater', 'chief', 'russian', 'argentina', 'to', 'include', 'syrian', 'with', 'attacks', 'of', 'rouhani']\n",
            "(e2):  ['sudanese', 'sues', 'says', 'boeing', 'pull', 'and', 'algeria', 'aide', 'tests', 'northern', 'chief', 'controls']\n",
            "(e3):  ['kurdish', 'u.n.', 'even', 'of', 'delayed', 'stone', 'after', 'standoff', 'cites', 'nigeria', 'orders', 'summit']\n",
            "2nd (e4): ['<bos>', 'kurdish', 'fighters', 'pull', 'out', 'of', 'flashpoint', 'town', ':', 'syria', \"'s\", 'defense', 'ministry', '<eos>']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Iter: 4; Temp: 0.79;\n",
            "1st (e0): ['<bos>', 'n.korea', \"'s\", 'kim', 'says', 'new', 'path', 'inevitable', 'if', 'u.s.', 'demands', 'unilateral', 'action', '<eos>']\n",
            "(e1):  ['n.korea', 'says', 'situation', 'state', 'new', 'economics', 'ukrainian', 'ties', 'ceasefire', 'indian', 'the']\n",
            "(e2):  ['mp', 'turkey', 'of', 'spirit', 'one', 'will', 'white', 'expulsion', 'crisis', 'no', 'ukraine', 'seeks']\n",
            "(e3):  ['kurdish', 'senior', 'bolsonaro', 'f-35', 'as', 'body', 'airport', 'war', 'sanctions', 'official', 'greenland', 'loans']\n",
            "2nd (e4): ['<bos>', 'kurdish', 'fighters', 'pull', 'out', 'of', 'flashpoint', 'town', ':', 'syria', \"'s\", 'defense', 'ministry', '<eos>']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Iter: 5; Temp: 0.79;\n",
            "1st (e0): ['<bos>', 'n.korea', \"'s\", 'kim', 'says', 'new', 'path', 'inevitable', 'if', 'u.s.', 'demands', 'unilateral', 'action', '<eos>']\n",
            "(e1):  ['n.korea', 'pm', 'sen.', 'new', 'any', 'syrian', 'striking', 'max', 'uranium', 'war', 'groups']\n",
            "(e2):  ['sudanese', 'diplomat', 'red-hot', 'as', 'pay', 'strait', 'g20', 'made', ':', 'east', 'awards', 'putin']\n",
            "(e3):  ['kurdish', 'campuses', 'likely', 'refugees', 'of', 'qualifier', 'weapon', 'envoy', 'nuclear', ':', 'yonhap']\n",
            "2nd (e4): ['<bos>', 'kurdish', 'fighters', 'pull', 'out', 'of', 'flashpoint', 'town', ':', 'syria', \"'s\", 'defense', 'ministry', '<eos>']\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
